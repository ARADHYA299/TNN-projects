# -*- coding: utf-8 -*-
"""TNN-AIPlagdetec.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1L28EmTBv2OMQb7EDIjZhqsARC4H-sekk
"""

!pip -q install datasets==2.20.0 scikit-learn==1.4.2 xgboost==2.0.3 lightgbm==4.3.0 matplotlib==3.8.4 joblib==1.3.2

from datasets import load_dataset
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import random,os

SEED = 42
random.seed(SEED);np.random.seed(SEED)

df = load_dataset("Hello-SimpleAI/HC3", "all" , trust_remote_code=True)

split_names = list(df.keys())
print(split_names)

data_split = "train" if "train" in split_names else split_names[0]
print(data_split)

raw = df[data_split]

rows = []
for ex in raw:
    q = ex.get("question", "")
    cat = ex.get("category", ex.get("source", "unknown"))

    # Human answers
    for h in ex.get("human_answers", []) or []:
        if isinstance(h, str) and h.strip():
            rows.append({"text": h.strip(), "label": 0, "category": cat, "question": q})

    # AI answers
    for a in ex.get("chatgpt_answers", []) or []:
        if isinstance(a, str) and a.strip():
            rows.append({"text": a.strip(), "label": 1, "category": cat, "question": q})

df = pd.DataFrame(rows).dropna(subset=["text"])
df = df[df["text"].str.len() > 10].reset_index(drop=True)

print("Total num. of Samples:", len(df))
print(df["label"].value_counts(), "\n")

x = df["text"].values
y = df["label"].values

from sklearn.model_selection import train_test_split
x_train,x_tmp,y_train,y_tmp = train_test_split(x,y,test_size=0.3,random_state=SEED,stratify = y)
x_val,x_test,y_val,y_test = train_test_split(x_tmp,y_tmp,test_size=0.5,random_state=SEED,stratify = y_tmp)

print("train",x_train.shape,"Val: ",x_val.shape,"Test: ",x_test.shape)

from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(ngram_range=(1,2),max_features = 200000, min_df = 2)
x_train_tfidf = vectorizer.fit_transform(x_train)
x_val_tfidf = vectorizer.transform(x_val)
x_test_tfidf = vectorizer.transform(x_test)

from sklearn.utils.class_weight import compute_class_weight

classes = np.unique(y_train)
cw = compute_class_weight(class_weight="balanced", classes=classes, y=y_train)
class_weight_dict = {cls: w for cls, w in zip(classes, cw)}
class_weight_dict

import numpy as np

print("Full dataset:", np.unique(y, return_counts=True))
print("Train:", np.unique(y_train, return_counts=True))
print("Val:", np.unique(y_val, return_counts=True))
print("Test:", np.unique(y_test, return_counts=True))

from sklearn.linear_model import LogisticRegression
from sklearn.utils.class_weight import compute_class_weight

classes = np.unique(y_train)
cw = compute_class_weight(class_weight="balanced", classes=classes, y=y_train)
class_weight_dict = {cls: w for cls, w in zip(classes, cw)}

lr = LogisticRegression(max_iter=2000, class_weight=class_weight_dict, n_jobs=-1)
lr.fit(x_train_tfidf, y_train)

import lightgbm as lgb

lgb_clf = lgb.LGBMClassifier(
    n_estimators=300,max_depth = -1,learning_rate=0.1,subsample = 0.8,colsample_bytree = 0.8,random_state = SEED,
    objective = "binary",class_weight = "balanced"
)

lgb_clf.fit(
    x_train_tfidf,y_train,eval_set = [(x_val_tfidf,y_val)],eval_metric = 'logloss',
    callbacks = [lgb.early_stopping(30),lgb.log_evaluation(50)]
)

from sklearn.metrics import roc_auc_score ,confusion_matrix,classification_report


y_val_pred = lgb_clf.predict(x_val_tfidf)
y_val_proba = lgb_clf.predict_proba(x_val_tfidf)[:,1]

print("Classification Report :\n",classification_report(y_val,y_val_pred))
print("ROC-AUC",roc_auc_score(y_val,y_val_proba))

results = lgb_clf.evals_result_
plt.plot(results['valid_0']['binary_logloss'] , label = 'Validation Logloss')
plt.xlabel("Iterations")
plt.ylabel("Logloss")
plt.title("LightGBM Validation Logloss Curve")
plt.legend()
plt.grid(True)
plt.show()

import joblib

joblib.dump(lgb_clf, "lightgbm_ai_detector.pkl")

joblib.dump(vectorizer, "tfidf_vectorizer.pkl")

print("Artifacts saved: lightgbm_ai_detector.pkl , tfidf_vectorizer.pkl")

lgb_model = joblib.load("lightgbm_ai_detector.pkl")
vectorizer = joblib.load("tfidf_vectorizer.pkl")

print("Artifacts loaded successfully")

def detect_ai_text(texts, model, vectorizer, threshold=0.5):
    """
    Predict whether given text(s) are AI-written or Human-written.
    texts: string or list of strings
    threshold: probability cutoff (default = 0.5)
    """
    if isinstance(texts, str):
        texts = [texts]


    X = vectorizer.transform(texts)


    probs = model.predict_proba(X)[:,1]

    results = []
    for t, p in zip(texts, probs):
        label = "AI-generated" if p >= threshold else "Human-written"
        results.append({
            "text_preview": t[:100] + ("..." if len(t) > 100 else ""),
            "probability_AI": round(float(p), 4),
            "prediction": label
        })
    return results

corpus = [
    "The mitochondria is the powerhouse of the cell.",  #human-Written
    "In conclusion, leveraging cutting-edge advancements in natural language processing enables scalable synergies."  #AI-written
]

results = detect_ai_text(corpus, lgb_model, vectorizer)

for r in results:
    print(r)

